{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTZ74veMIX6Q"
      },
      "source": [
        "# EE 467 Lab 6: Ensemble Learning and Random Forest\n",
        "\n",
        "Welcome to lab 6 of EE 467! Today we are going to learn and try out **ensemble learning** algorithms, which make use of multiple learning algorithms to achieve better performance than any of them. We will apply four kinds of common ensembles to the Kaggle credit card fraud detection problem: **voting, bagging, boosting and stacking**. We will also try **random forest** learning, which is a special kind of bagging ensemble that consists of decision trees. Like the previous lab, all algorithms are evaluated by **accuracy, precision, recall and F1-score**.\n",
        "\n",
        "## Pre-processing / Feature Extraction\n",
        "\n",
        "Let's start from the end of lab 5. First of all, we will load the credit card transaction dataset and re-do all the feature scaling and dataset splitting steps in the last lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpcW1lO0P0Yz"
      },
      "outputs": [],
      "source": [
        "!tar -xf credit-card.tar.xz #<--- To Unzip data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UIDzoEQVIX6R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Kaggle credit card transactions dataset...\n",
            "Scaling transaction time and amount features...\n",
            "Performing feature-label / train-test splits...\n",
            "Completed.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "## [ Loading Dataset ]\n",
        "print(\"Loading Kaggle credit card transactions dataset...\")\n",
        "df = pd.read_csv(\"./creditcard.csv\")\n",
        "\n",
        "## [ Feature Scaling ]\n",
        "print(\"Scaling transaction time and amount features...\")\n",
        "# Convert transaction time from seconds to hours in a day\n",
        "df[\"Time\"] = (df[\"Time\"]/(60*60))%24\n",
        "# Scale time features with StandardScaler (suitable for normally distributed data)\n",
        "df[\"Time\"] = StandardScaler().fit_transform(df[\"Time\"].values[:, None])\n",
        "# Scale amount with RobustScaler (robust to outliers, useful for skewed distributions)\n",
        "df[\"Amount\"] = RobustScaler().fit_transform(df[\"Amount\"].values[:, None])\n",
        "\n",
        "## [ Feature-label / train-test splits ]\n",
        "print(\"Performing feature-label / train-test splits...\")\n",
        "\n",
        "# Get feature and label values from original dataset\n",
        "feat_all = df.drop([\"Class\"], axis=1).values\n",
        "y_all = df[\"Class\"].values\n",
        "\n",
        "# Split samples into training and test sets\n",
        "feat_train, feat_test, y_train, y_test = train_test_split(\n",
        "    feat_all, y_all, test_size=0.4, random_state=0\n",
        ")\n",
        "\n",
        "print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us6s354jIX6S"
      },
      "source": [
        "During this lab we will use two utility functions from `lab_7_util`. The `timeit` function, which you should be fairly familiar with, times Python operations happening within the corresponding `with` block. The `evaluate_model` function evaluates a trained classification model on the test set and then prints the above-mentioned metrics we are interested about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PzxP3t4kIX6S",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training logistic regression classifier started...\n",
            "Training logistic regression classifier completed. Elapsed time: 0.36s\n",
            "\n",
            "[ Evaluation result for logistic regression classifier ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.59      0.71       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.95      0.80      0.86    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    81    118]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from lab_6_util import timeit, evaluate_model\n",
        "\n",
        "# Time the training of logistic regression classifier\n",
        "with timeit(\"Training logistic regression classifier\"):\n",
        "    logistic_model = LogisticRegression(max_iter=200).fit(feat_train, y_train)\n",
        "\n",
        "# Evaluate trained model and print metrics\n",
        "evaluate_model(logistic_model, \"logistic regression classifier\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2Y-Yo4IX6S"
      },
      "source": [
        "## Voting\n",
        "\n",
        "The simplest kind of ensemble is a **voting ensemble**. Like a group of people making decisions through a majority vote, a voting ensemble contains multiple classification models, usually implemented from different algorithms. During training, each model learns independently from others. To make a prediction using the ensemble, each model \"votes\" by providing its own prediction computed from the sample features. The final predicted label of the ensemble is then the class with most \"votes\" from different models.\n",
        "\n",
        "For classification models that output a probability distribution over all classes, there is an alternative voting mechanism called **soft voting**. In soft voting, we average the probability of a particular class for all classifiers, and refer to it as the ensemble probability of a class. The ensemble prediction is then the class with the highest ensemble probability. Soft voting largely avoids the **tie-breaking problem** of hard voting, in which two or more majority classes exist with the same number of votes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u8SfGkVsIX6T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ Evaluation result for voting ensemble ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.79      0.77      0.78       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.90      0.88      0.89    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113684     40]\n",
            " [    46    153]] \n",
            "\n",
            "[ Evaluation result for Logistic ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.59      0.71       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.95      0.80      0.86    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    81    118]] \n",
            "\n",
            "[ Evaluation result for Naive Bayes ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99    113724\n",
            "           1       0.06      0.83      0.12       199\n",
            "\n",
            "    accuracy                           0.98    113923\n",
            "   macro avg       0.53      0.91      0.55    113923\n",
            "weighted avg       1.00      0.98      0.99    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[111328   2396]\n",
            " [    33    166]] \n",
            "\n",
            "[ Evaluation result for Decision Tree ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.75      0.73      0.74       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.88      0.87      0.87    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113676     48]\n",
            " [    53    146]] \n",
            "\n",
            "[ Evaluation result for voting ensemble ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.81      0.76      0.78       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.91      0.88      0.89    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113689     35]\n",
            " [    48    151]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Create and evaluate a voting ensemble (`VotingClassifier`) with the following sub-classifiers:\n",
        "#    - A logistic regression classifier\n",
        "#      (Hint: increase maximum iteration to 200 to avoid non-convergence warning)\n",
        "#    - A Gaussian naive Bayes (`GaussianNB`) classifier\n",
        "#    - A decision tree classifier\n",
        "voting_ensemble = VotingClassifier([\n",
        "    ('Logistic', LogisticRegression(max_iter=200)),\n",
        "    ('Naive Bayes', GaussianNB()),\n",
        "    ('Decision Tree', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "voting_ensemble.fit(feat_train, y_train)\n",
        "\n",
        "# Evaluate the voting ensemble classifier\n",
        "evaluate_model(voting_ensemble, 'voting ensemble', feat_test, y_test)\n",
        "\n",
        "# 2) Evaluate the performance of each sub-classifier on the test set\n",
        "#    (Hint: obtain the sub-classifiers through `voting_ensemble.named_estimators_`)\n",
        "for (name, model) in voting_ensemble.named_estimators_.items():\n",
        "    evaluate_model(model, name, feat_test, y_test)\n",
        "\n",
        "# 3) Change the voting mechanism to soft voting. Does the performance of the ensemble improved?\n",
        "voting_ensemble = VotingClassifier([\n",
        "    ('Logistic', LogisticRegression(max_iter=200)),\n",
        "    ('Naive Bayes', GaussianNB()),\n",
        "    ('Decision Tree', DecisionTreeClassifier())\n",
        "], voting='soft')\n",
        "\n",
        "voting_ensemble.fit(feat_train, y_train)\n",
        "\n",
        "evaluate_model(voting_ensemble, 'voting ensemble', feat_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, switching to soft voting appears to have had little impact on performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOZ0xVfIX6U"
      },
      "source": [
        "## Bagging / Random Forest\n",
        "\n",
        "**Bagging ensemble** is a kind of ensemble built upon voting. Compared to regular voting ensembles, a bagging ensemble only contains several classifiers of the **same type** (implementing the same algorithm and using the same hyper-parameter settings), each of which is trained on **a random subset of samples (and / or features)**. Bagging reduces over-fitting of the original classification model by introducing randomization into its construction and then making an ensemble out of it. It works best with strong and complex machine learning models such as neural networks and deep decision trees.\n",
        "\n",
        "In the following code cell, we will train a few **logistic regression bagging ensemble** with different settings. We will alter the number of (sub-)classifiers, proportion of samples and features and study their influence on the performance of the bagging ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "o2y_CV9yIX6V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training bagging ensemble (10x 20%) started...\n",
            "Training bagging ensemble (10x 20%) completed. Elapsed time: 3.11s\n",
            "\n",
            "[ Evaluation result for bagging ensemble (10x 20%) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.55      0.68       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.94      0.78      0.84    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    89    110]] \n",
            "\n",
            "Training bagging ensemble (30x 20%) started...\n",
            "Training bagging ensemble (30x 20%) completed. Elapsed time: 10.46s\n",
            "\n",
            "[ Evaluation result for bagging ensemble (30x 20%) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.59      0.71       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.95      0.80      0.86    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    81    118]] \n",
            "\n",
            "Training bagging ensemble (10x 60%) started...\n",
            "Training bagging ensemble (10x 60%) completed. Elapsed time: 4.80s\n",
            "\n",
            "[ Evaluation result for bagging ensemble (10x 60%) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.58      0.71       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.95      0.79      0.85    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    83    116]] \n",
            "\n",
            "Training bagging ensemble (10x 50%; bootstrapped features) started...\n",
            "Training bagging ensemble (10x 50%; bootstrapped features) completed. Elapsed time: 4.56s\n",
            "\n",
            "[ Evaluation result for bagging ensemble (10x 50%; bootstrapped features) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.58      0.70       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.95      0.79      0.85    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113710     14]\n",
            " [    84    115]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Create and evaluate a logistic regression bagging ensemble (`BaggingClassifier`)\n",
        "#    with 10 sub-classifiers, each using 20% of samples.\n",
        "# 2) Based on the ensemble of question 1, increase the number of sub-classifiers to 30\n",
        "#    and evaluate again.\n",
        "# 3) Based on the ensemble of question 1, increase the proportion of training samples for\n",
        "#    each classifier to 60% and evaluate again.\n",
        "# 4) Based on the ensemble of question 1, enable boostrapping of features, set the\n",
        "#    proportion of features to 50% and evaluate again.\n",
        "bagging_models = {\n",
        "    '10x 20%': BaggingClassifier(\n",
        "        LogisticRegression(), \n",
        "        n_estimators=10, \n",
        "        max_samples=0.2\n",
        "    ),\n",
        "    '30x 20%': BaggingClassifier(\n",
        "        LogisticRegression(),\n",
        "        n_estimators=30,\n",
        "        max_samples=0.2\n",
        "    ),\n",
        "    '10x 60%': BaggingClassifier(\n",
        "        LogisticRegression(),\n",
        "        n_estimators=10,\n",
        "        max_samples=0.6\n",
        "    ),\n",
        "    '10x 50%; bootstrapped features': BaggingClassifier(\n",
        "        LogisticRegression(),\n",
        "        n_estimators=10,\n",
        "        max_samples=0.5,\n",
        "        bootstrap_features=True\n",
        "    )\n",
        "}\n",
        "\n",
        "for setting, model in bagging_models.items():\n",
        "    # Train each bagging classifier\n",
        "    with timeit(f\"Training bagging ensemble ({setting})\"):\n",
        "        model.fit(feat_train, y_train)\n",
        "    # Evaluate each bagging classifier\n",
        "    evaluate_model(model, f\"bagging ensemble ({setting})\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q209h-1wIX6V"
      },
      "source": [
        "In practice, bagging ensemble is often built upon decision tree classifiers. When each decision tree within the ensemble learns from **both a subset of samples and a subset of features**, the resulting bagging ensemble is called a **random forest**. Below code trains and compares the performance of a single decision tree and two random forests with 40 and 100 decision tree classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ME6ks2a3IX6W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DT classifier started...\n",
            "Training DT classifier completed. Elapsed time: 9.41s\n",
            "\n",
            "[ Evaluation result for DT classifier ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.74      0.75      0.74       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.87      0.87      0.87    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113671     53]\n",
            " [    50    149]] \n",
            "\n",
            "[ Evaluation result for Random forest classifier (40 DTs) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.95      0.77      0.85       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.97      0.88      0.92    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113716      8]\n",
            " [    46    153]] \n",
            "\n",
            "[ Evaluation result for Random forest classifier (100 DTs) ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.94      0.77      0.85       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.97      0.88      0.92    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113715      9]\n",
            " [    46    153]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Number of CPUs for ensemble learning methods\n",
        "N_ENSEMBLE_CPUS = max(os.cpu_count()//2, 1)\n",
        "\n",
        "# A regular decision tree classifier\n",
        "with timeit(\"Training DT classifier\"):\n",
        "    dt_model = DecisionTreeClassifier()\n",
        "    dt_model.fit(feat_train, y_train)\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Train a random forest classifier with 40 decision trees\n",
        "# 2) Train a random forest classifier with 100 decision trees\n",
        "#    (Hint: set `n_jobs` to `N_ENSEMBLE_CPUS` to train the random forest in parallel and reduce training time)\n",
        "rf_40_model = RandomForestClassifier(\n",
        "    n_estimators=40,\n",
        "    n_jobs=N_ENSEMBLE_CPUS\n",
        ")\n",
        "rf_100_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    n_jobs=N_ENSEMBLE_CPUS\n",
        ")\n",
        "\n",
        "rf_40_model.fit(feat_train, y_train)\n",
        "rf_100_model.fit(feat_train, y_train)\n",
        "\n",
        "# Evaluate previous models\n",
        "evaluate_model(dt_model, \"DT classifier\", feat_test, y_test)\n",
        "evaluate_model(rf_40_model, \"Random forest classifier (40 DTs)\", feat_test, y_test)\n",
        "evaluate_model(rf_100_model, \"Random forest classifier (100 DTs)\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxMyBPQBIX6W"
      },
      "source": [
        "## Boosting\n",
        "\n",
        "Boosting is an ensemble learning technique that aims to combine **a set of weak learners** (a classifier that is only **slightly better** than a random classifier) into a strong learner. It is able to reduce both the bias and variance of the original classification models. A boosting algorithm usually consists of **iteratively learning weak classifiers** with respect to a distribution and **adding them to a final strong classifier**. Weak classifiers are typically weighted in some way that is related to its performance. After a weak learner is added, sample weights are re-adjusted so that **misclassified samples are stressed** and correctly classified samples are paid less attention to. This causes future weak learners to focus more on samples that previous weak learners fail, thus making the ensemble more robusting against variations in sample features.\n",
        "\n",
        "In the following code cell, we will try two kinds of boosting ensembles: **[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) and [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)**. Both use one-level (as opposed to full, deep) decision trees as base weak classifiers. AdaBoost adjusts the weights of training samples and weak classifiers based on the accuracy, while gradient boosting adjusts the weights by differentiating through the target loss and computing the correponsing gradient for gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JE1-tgJkIX6W",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training AdaBoost classifier (50 DTs) started...\n",
            "Training AdaBoost classifier (50 DTs) completed. Elapsed time: 33.39s\n",
            "\n",
            "Training gradient boosting classifier started...\n",
            "Training gradient boosting classifier completed. Elapsed time: 80.51s\n",
            "\n",
            "[ Evaluation result for AdaBoost classifier ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.80      0.70      0.75       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.90      0.85      0.87    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113688     36]\n",
            " [    59    140]] \n",
            "\n",
            "[ Evaluation result for gradient boosting classifier ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.89      0.60      0.72       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.94      0.80      0.86    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113709     15]\n",
            " [    79    120]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "\n",
        "# AdaBoost: adjusts weights based on misclassification rates (50 default weak learners)\n",
        "with timeit(\"Training AdaBoost classifier (50 DTs)\"):\n",
        "    adaboost_model = AdaBoostClassifier()\n",
        "    adaboost_model.fit(feat_train, y_train)\n",
        "\n",
        "# Gradient boosting: sequentially corrects errors using gradient descent (40 estimators)\n",
        "with timeit(\"Training gradient boosting classifier\"):\n",
        "    gb_model = GradientBoostingClassifier(n_estimators=40)\n",
        "    gb_model.fit(feat_train, y_train)\n",
        "\n",
        "# Evaluate boosting models\n",
        "evaluate_model(adaboost_model, \"AdaBoost classifier\", feat_test, y_test)\n",
        "evaluate_model(gb_model, \"gradient boosting classifier\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dBxGmOCIX6X"
      },
      "source": [
        "## Stacking\n",
        "\n",
        "The last ensemble learning scheme is **stacking**, which is also the most sophisticated one among the four we have introduced today. Like all previous types of ensemble, a stacking ensemble contains a number of classifiers, each possibly using a distinct machine learning algorithm. However, we would perform $K$-fold **cross validation** during training on each classifier, so each classifier actually has $K$ clones that are trained on different parts of the training samples. The cross validation also provides us with the **validation predictions for all samples**, gathered from the $K$ clones. After training all base classifiers, we collect and concatenate the validation predictions from clones of different classifiers as features, and then **train a meta-classifier** that predicts the sample label for the whole ensemble.\n",
        "\n",
        "\n",
        "To predict labels for the test set (and any other unseen dataset) samples, we apply the features to all $K$ clones of all classifiers. For the $K$ clones of the same classifier, we **average their outputs** which are usually the probability distribution over all classes. Like the training data, we then **concatenate the averaged outputs** from different classifiers as features, and finally pass them to the meta-classifier to obtain predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hubsNFsAIX6X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training stacking ensemble started...\n",
            "Training stacking ensemble completed. Elapsed time: 103.41s\n",
            "\n",
            "[ Evaluation result for stacking ensemble ]\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    113724\n",
            "           1       0.94      0.74      0.83       199\n",
            "\n",
            "    accuracy                           1.00    113923\n",
            "   macro avg       0.97      0.87      0.91    113923\n",
            "weighted avg       1.00      1.00      1.00    113923\n",
            "\n",
            "Confusion matrix:\n",
            "[[113715      9]\n",
            " [    52    147]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "with timeit(\"Training stacking ensemble\"):\n",
        "    # Create a stacking ensemble with a logistic regression meta-classifier and three sub-classifiers\n",
        "    stacking_ensemble = StackingClassifier([\n",
        "        (\"Random forest\", RandomForestClassifier(n_estimators=40)),\n",
        "        (\"Logistic\", LogisticRegression(max_iter=200)),\n",
        "        (\"SVM\", LinearSVC(max_iter=1500))\n",
        "    ], LogisticRegression(), n_jobs=N_ENSEMBLE_CPUS)\n",
        "    # Train the stacking ensemble\n",
        "    stacking_ensemble.fit(feat_train, y_train)\n",
        "\n",
        "# Evaluate the stacking ensemble\n",
        "evaluate_model(stacking_ensemble, \"stacking ensemble\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMhCX9MbIX6X",
        "tags": []
      },
      "source": [
        "## References\n",
        "\n",
        "1. Emsemble Learning: https://en.wikipedia.org/wiki/Ensemble_learning\n",
        "2. Ensemble Learning in Machine Learning: https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00\n",
        "3. Random Forest: https://en.wikipedia.org/wiki/Random_forest\n",
        "4. Boosting: https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
        "5. AdaBoost: https://en.wikipedia.org/wiki/AdaBoost\n",
        "6. Gradient Boosting: https://en.wikipedia.org/wiki/Gradient_boosting"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
